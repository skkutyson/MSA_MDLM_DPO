# MSAGPT

<table>
  <tr>
    <td>
      <h2>MSAGPT</h2>
      <p>ðŸ“– Paper: <a href="https://arxiv.org/abs/2406.05347">MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training</a></p>
      <p><b>MSAGPT</b> is a powerful protein language model (PLM). MSAGPT has 3 billion parameters with three versions of the model, MSAGPT, MSAGPT-Sft, and MSAGPT-Dpo, <b>supporting zero-shot and few-shot MSA generation</b>.</p>
      <p><b>MSAGPT achieves state-of-the-art structural prediction performance on natural MSA-scarce scenarios</b>.</p>
    </td>
  </tr>
</table>


## Overall Framework
<p align="center">
<img src="resources/overall_frame.png" alt="æè¿°æ–‡å­—" style="display: block; margin: auto; width: 90%;">
</p>

## Visualized Cases
Visualization of improved structure prediction compared with nature MSA.
<font color=orange>Yellow</font>: Ground truth; 
<font color=purple>Purple</font>: Predictions based on MSA generated by MSAGPT; 
<font color=cyan>Cyan</font>: Predictions from MSA generated by natural MSA.

<p align="center">
<img src="resources/app_case.png" alt="æè¿°æ–‡å­—" style="display: block; margin: auto; width: 90%;">
</p>


## Get Started: 

### Option 1ï¼šDeploy MSAGPT by yourself

We support GUI for model inference.

First, we need to install the dependencies.

```bash
# CUDA >= 11.8
pip install -r requirements.txt
```

#### Model List
You can choose to manually download the necessary weights. Then UNZIP it and put it into the **checkpoints** folder.

| Model            | Type | Seq Length | Download                                                                                                                                |                                                                                                                                                                                
|------------------|------|------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| MSAGPT         | Base | 16K         | [ðŸ¤— Huggingface](https://huggingface.co/THUDM/MSAGPT)  [ðŸ”¨ SwissArmyTransformer](https://cloud.tsinghua.edu.cn/f/ebfc954a4cd24cef9243/?dl=1)  |
| MSAGPT-SFT   | Sft | 16K        | [ðŸ¤— Huggingface](https://huggingface.co/THUDM/MSAGPT)  [ðŸ”¨ SwissArmyTransformer](https://cloud.tsinghua.edu.cn/f/32da3eadf6e042aab2fa/?dl=1)   |
| MSAGPT-DPO | Rlhf | 16K         | [ðŸ¤— Huggingface](https://huggingface.co/THUDM/MSAGPT)  [ðŸ”¨ SwissArmyTransformer](https://cloud.tsinghua.edu.cn/f/ebfc954a4cd24cef9243/?dl=1) |                                                                                                                                                                                      |                                                                                                                                                                                  |


#### Situation 1.1 CLI (SAT version)

Run CLI demo via:

```bash
# Online Chat
bash scripts/cli_sat.sh --from_pretrained ./checkpoints/MSAGPT-DPO --input-source chat --stream_chat --max-gen-length 1024
```

The program will automatically interact in the command line. You can generate replies entering the protein sequence you need to generate virtual MSAs (or add a few MSAs as a prompt, connected by "\<M\>"), for example: "PEGKQGDPGIPGEPGPPGPPGPQGARGPPG\<M\>VTVEFVNSCLIGDMGVDGPPGQQGQPGPPG", where "PEGKQGDPGIPGEPGPPGPPGPQGARGPPG" is the main sequence, and "VTVEFVNSCLIGDMGVDGPPGQQGQPGPPG" are MSA prompts, and pressing enter. Enter `stop` to stop the program. The chat CLI looks like:
<p align="center">
<img src="resources/demo.gif" alt="æè¿°æ–‡å­—" style="display: block; margin: auto; width: 90%;">
</p>


You can also enable the offline generation by set the **--input-source \<your input file\>** and **--output-path \<your output path\>**.
We set an input file example: *msa_input*. 
```bash
# Offline Generation
bash scripts/cli_sat.sh --from_pretrained ./checkpoints/MSAGPT-DPO --input-source <your input file> --output-path <your output path> --max-gen-length 1024
```

#### Situation 1.2 CLI with MDLM Backbone (Diffusion-based)

The CLI also supports MDLM (Masked Diffusion Language Model) as an alternative backbone for MSA generation. Unlike autoregressive generation, MDLM generates all positions in parallel through iterative denoising.

```bash
# Online Chat with MDLM
bash scripts/cli_sat.sh \
    --from_pretrained ./checkpoints/mdlm_dpo \
    --backbone mdlm \
    --input-source chat \
    --stream_chat \
    --max-gen-length 512 \
    --num-diffusion-steps 256 \
    --diffusion-sampler ddpm_cache
```

```bash
# Offline Generation with MDLM
bash scripts/cli_sat.sh \
    --from_pretrained ./checkpoints/mdlm_dpo \
    --backbone mdlm \
    --input-source <your input file> \
    --output-path <your output path> \
    --max-gen-length 512 \
    --num-diffusion-steps 256
```

**MDLM-specific parameters:**
| Parameter | Description | Default |
|-----------|-------------|---------|
| `--backbone mdlm` | Use MDLM diffusion backbone | `gpt` |
| `--num-diffusion-steps` | Number of denoising steps (higher = better quality) | 256 |
| `--diffusion-sampler` | Sampling method: `ddpm` or `ddpm_cache` (faster) | `ddpm_cache` |

#### Few-Shot MSA Generation Mode

Both GPT and MDLM backbones support **few-shot learning** where you provide a few example MSAs to guide generation.

**How it works:**
- Provide 2-3 example MSAs as context (separated by `<M>`)
- The model conditions on these examples to generate similar MSAs
- Improves quality when natural MSAs have specific patterns

**Example with GPT backbone:**
```bash
# Online chat with few-shot prompt
bash scripts/cli_sat.sh \
    --from_pretrained ./checkpoints/MSAGPT-DPO \
    --input-source chat \
    --max-gen-length 1024

# Enter query with examples (in chat):
# PEGKQGDPGIPGEPGPPGPPGPQGARGPPG<M>VTVEFVNSCLIGDMGVDGPPGQQGQPGPPG<M>PEGKQGEPGIPGEPGPPGPPGPQGARGQPG
```

**Example with MDLM backbone:**
```bash
# Create input file with few-shot examples
echo "MVLKKVDPL<M>MVLKKVD--<M>MVLKAVDPL" > msa_fewshot.txt

# Generate with MDLM
bash scripts/cli_sat.sh \
    --from_pretrained ./checkpoints/mdlm_dpo \
    --backbone mdlm \
    --input-source msa_fewshot.txt \
    --output-path ./output_fewshot.txt \
    --max-gen-length 512 \
    --num-diffusion-steps 256
```

**Format:**
- `<Query Sequence><M><Example MSA 1><M><Example MSA 2><M>...`
- The model will generate additional MSAs following the pattern of your examples
- Recommended: 2-3 examples for best results

**When to use few-shot:**
- You have natural MSAs but want more diversity
- The protein has specific conserved regions you want to maintain
- You want to guide generation with known homologs

#### Situation 1.3 CLI (Huggingface version)
(TODO)

#### Situation 1.4 Web Demo
(TODO)

### Option 2: Training MDLM (Masked Diffusion Language Model)

This repository includes training infrastructure for MDLM with DPO (Direct Preference Optimization) for protein MSA generation.

#### 2.1 Data Preparation

Download and process OpenProteinSet data:

```bash
# Download MSA files (requires AWS CLI: pip install awscli)
python scripts/prepare_dataset.py download --output ./data/openproteinset --max-files 1000

# Process and filter MSAs (outputs parquet format for fast loading)
python scripts/prepare_dataset.py process \
    --input ./data/openproteinset \
    --output ./data/processed/msa_data.parquet

# Generate preference pairs for DPO (after pre-training)
python scripts/prepare_dataset.py generate-preferences \
    --model-path ./checkpoints/mdlm_pretrain/last.ckpt \
    --input ./data/processed/msa_data.parquet \
    --output ./data/preference_pairs.jsonl
```

#### 2.2 Pre-training

Train the MDLM model on MSA data:

```bash
# Using parquet format (10-100x faster loading)
python scripts/train_mdlm.py --config configs/train_mdlm.yaml \
    --data.path ./data/processed/msa_data.parquet \
    --training.max_steps 100000

# Or using raw A3M directory (slower)
python scripts/train_mdlm.py --config configs/train_mdlm.yaml \
    --data.path ./data/openproteinset \
    --training.max_steps 100000
```

**Fast loader (recommended):**
- Parquet uses memory-mapped loading (10-100x faster than raw A3M).
- Output schema: `id`, `query`, `sequences_json`, `depth`, `length`.

**FlashAttention (optional, faster attention):**
```bash
pip install flash-attn --no-build-isolation
```
If `flash_attn` is installed, it is auto-detected and used in the MDLM backbone.

**Sampling time direction (important):**
- Noise schedule follows standard convention: $t$ increases â†’ $\sigma(t)$ increases.
- **Sampling/denoising runs backward in time:** start at $t=1$ (fully masked) and step to $t=0$ (clean).
- Re-masking uses $\sigma_{t-\Delta t}$ directly (lower $\sigma$ â†’ fewer re-masks).

Key hyperparameters (from MSAGPT paper):
- Batch size: 48 MSAs, 12,288 residues per batch
- Learning rate: 1.2e-4 with cosine schedule
- Optimizer: AdamW (beta1=0.9, beta2=0.95)

#### 2.3 DPO Fine-tuning

Fine-tune with Direct Preference Optimization:

```bash
python scripts/train_mdlm_dpo.py --config configs/train_dpo.yaml \
    --model.checkpoint_path ./checkpoints/mdlm_pretrain/last.ckpt \
    --data.path ./data/preference_pairs.jsonl
```

DPO hyperparameters:
- Learning rate: 1e-6
- Beta (DPO temperature): 0.1
- Lambda (CE regularization): 0.1

#### 2.4 Training Infrastructure

```
training/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ openproteinset_loader.py    # OpenProteinSet A3M/FASTA parser
â”‚   â”œâ”€â”€ msa_dataset.py              # Pre-training dataset with 2D positions
â”‚   â”œâ”€â”€ msa_preference_dataset.py   # DPO preference pairs dataset
â”‚   â””â”€â”€ preference_generator.py     # Generate pairs using proxy metrics
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ mdlm_loss.py                # Diffusion NLL loss (SUBS parameterization)
â”‚   â””â”€â”€ mdlm_dpo_loss.py            # D3PO loss for masked diffusion
â”œâ”€â”€ trainers/
â”‚   â”œâ”€â”€ mdlm_trainer.py             # PyTorch Lightning pre-training module
â”‚   â””â”€â”€ mdlm_dpo_trainer.py         # DPO fine-tuning module
â””â”€â”€ utils/
    â”œâ”€â”€ ema.py                      # Exponential Moving Average
    â””â”€â”€ metrics.py                  # Perplexity, accuracy metrics
```

### Hardware requirement

* Model Inference:
  For BF16: 1 * A100(80G) 

* Finetuning:

  For BF16: 4 * A100(80G) *[Recommend]*.


## Natural MSA-scarce benchmark
Please find the 199 cases along with their retrieved MSAs in the [natural-msa-scarce-cases.txt](./natural-msa-scarce-cases.txt) file. Each line is structured as follows:
```
<PDB-id> <Primary Sequence> <M> <MSA1> <M> ... <MSAn>
```

### Explanation of the Data Structure
- **`<PDB-id>`**: The unique identifier for the protein structure.
- **`<Primary Sequence>`**: The amino acid sequence of the protein.
- **`<M>`**: A delimiter separating different sections.
- **`<MSA1> ... <MSAn>`**: The multiple sequence alignments retrieved for each case.



## Documentation

- [MDLM Integration Technical Documentation](./docs/MDLM_INTEGRATION.md) - Detailed technical overview of how MDLM was integrated into MSAGPT

## License

The code in this repository is open source under the [Apache-2.0 license](./LICENSE).

If you find our work helpful, please consider citing the our paper

```
@article{chen2024msagpt,
  title={MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training},
  author={Chen, Bo and Bei, Zhilei and Cheng, Xingyi and Li, Pan and Tang, Jie and Song, Le},
  journal={arXiv preprint arXiv:2406.05347},
  year={2024}
}
```